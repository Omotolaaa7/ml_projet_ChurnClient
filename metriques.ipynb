{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f969f422",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Guide Complet des Métriques de Machine Learning\n",
    "\n",
    "## Table des matières\n",
    "1. [Introduction](#introduction)\n",
    "2. [Métriques de Classification](#métriques-de-classification)\n",
    "3. [Métriques de Régression](#métriques-de-régression)\n",
    "4. [Métriques de Clustering](#métriques-de-clustering)\n",
    "5. [Métriques pour Problèmes Multi-Classes](#métriques-multi-classes)\n",
    "6. [Bibliothèques et Imports](#bibliothèques-et-imports)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Les métriques de Machine Learning sont essentielles pour évaluer les performances de vos modèles. Le choix de la métrique dépend du type de problème, du contexte métier et de l'équilibre souhaité entre différents types d'erreurs.\n",
    "\n",
    "---\n",
    "\n",
    "## Métriques de Classification\n",
    "\n",
    "### 1. Matrice de Confusion\n",
    "\n",
    "**Utilité** : Base de toutes les métriques de classification binaire\n",
    "\n",
    "**Structure** :\n",
    "```\n",
    "                 Prédiction\n",
    "              Positif  Négatif\n",
    "Réel Positif    TP       FN\n",
    "     Négatif    FP       TN\n",
    "```\n",
    "\n",
    "- **TP (True Positive)** : Vrais positifs\n",
    "- **TN (True Negative)** : Vrais négatifs\n",
    "- **FP (False Positive)** : Faux positifs (Erreur de Type I)\n",
    "- **FN (False Negative)** : Faux négatifs (Erreur de Type II)\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Générer la matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualisation\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.ylabel('Vraies valeurs')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Accuracy (Exactitude)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Utilité** : Proportion de prédictions correctes parmi toutes les prédictions\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "-  Classes équilibrées\n",
    "-  Coût égal des faux positifs et faux négatifs\n",
    "\n",
    "**Quand NE PAS l'utiliser** :\n",
    "-  Classes déséquilibrées (ex: détection de fraude avec 1% de fraudes)\n",
    "-  Lorsqu'un type d'erreur est plus coûteux que l'autre\n",
    "\n",
    "**Interprétation** : \n",
    "- Valeur entre 0 et 1 (ou 0% et 100%)\n",
    "- Plus élevée = meilleure performance\n",
    "- Ex: 0.95 = 95% de prédictions correctes\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Precision (Précision)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Utilité** : Parmi les éléments prédits positifs, quelle proportion est réellement positive ?\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "-  Minimiser les faux positifs est critique\n",
    "-  Détection de spam (éviter de classer des emails importants comme spam)\n",
    "-  Recommandation de produits (éviter de recommander des produits non pertinents)\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre 0 et 1\n",
    "- Haute précision = peu de faux positifs\n",
    "- Ex: Precision = 0.9 → 90% des prédictions positives sont correctes\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Pour multi-classes\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Recall (Rappel / Sensibilité / True Positive Rate)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Utilité** : Parmi tous les éléments réellement positifs, quelle proportion a été correctement identifiée ?\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "-  Minimiser les faux négatifs est critique\n",
    "-  Diagnostic médical (ne pas manquer de maladies)\n",
    "-  Détection de fraude (capturer toutes les fraudes)\n",
    "-  Détection d'objets critiques (défauts de fabrication)\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre 0 et 1\n",
    "- Haut recall = peu de faux négatifs\n",
    "- Ex: Recall = 0.85 → 85% des cas positifs sont détectés\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. F1-Score\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$$\n",
    "\n",
    "**Utilité** : Moyenne harmonique entre Precision et Recall\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Classes déséquilibrées\n",
    "- Besoin d'équilibre entre Precision et Recall\n",
    "- Métrique unique pour comparer des modèles\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre 0 et 1\n",
    "- F1 = 1 → Precision et Recall parfaits\n",
    "- F1 est pénalisé si Precision ou Recall est faible\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Variantes\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. F-Beta Score\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "**Utilité** : Généralisation du F1-Score permettant de pondérer Precision vs Recall\n",
    "\n",
    "**Paramètre β** :\n",
    "- β < 1 : Favorise la Precision\n",
    "- β = 1 : F1-Score (équilibre)\n",
    "- β > 1 : Favorise le Recall\n",
    "- β = 2 : F2-Score (Recall 2× plus important)\n",
    "- β = 0.5 : F0.5-Score (Precision 2× plus important)\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- F2 pour détection médicale (privilégier Recall)\n",
    "- F0.5 pour filtres anti-spam (privilégier Precision)\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# F2-Score (favorise Recall)\n",
    "f2 = fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# F0.5-Score (favorise Precision)\n",
    "f05 = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "\n",
    "print(f\"F2-Score: {f2:.4f}\")\n",
    "print(f\"F0.5-Score: {f05:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ROC AUC (Area Under ROC Curve)\n",
    "\n",
    "**Utilité** : Mesure la capacité du modèle à distinguer entre les classes\n",
    "\n",
    "**Composantes de la courbe ROC** :\n",
    "- **Axe X** : False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- **Axe Y** : True Positive Rate (TPR) = TP / (TP + FN) = Recall\n",
    "\n",
    "**Formule AUC** : Aire sous la courbe ROC\n",
    "\n",
    "**Interprétation** :\n",
    "- AUC = 0.5 : Modèle aléatoire (inutile)\n",
    "- 0.5 < AUC < 0.7 : Performance faible\n",
    "- 0.7 ≤ AUC < 0.8 : Performance acceptable\n",
    "- 0.8 ≤ AUC < 0.9 : Performance bonne\n",
    "- AUC ≥ 0.9 : Performance excellente\n",
    "- AUC = 1.0 : Modèle parfait\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Évaluer les modèles probabilistes\n",
    "- Comparer différents modèles\n",
    "- Classes déséquilibrées\n",
    "- Indépendant du seuil de classification\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer AUC\n",
    "auc = roc_auc_score(y_true, y_pred_proba)\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Precision-Recall AUC\n",
    "\n",
    "**Utilité** : Alternative au ROC AUC pour les classes très déséquilibrées\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Classes très déséquilibrées (ex: 1% de positifs)\n",
    "- Focus sur la classe minoritaire positive\n",
    "- ROC AUC peut être trop optimiste\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre 0 et 1\n",
    "- Plus élevée = meilleure performance\n",
    "- Baseline = proportion de la classe positive\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "# Calculer PR AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Tracer la courbe Precision-Recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Log Loss (Cross-Entropy Loss)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Log Loss} = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$\n",
    "\n",
    "**Utilité** : Mesure la qualité des probabilités prédites\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Plus faible = meilleure performance\n",
    "- Pénalise fortement les prédictions confiantes mais fausses\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Évaluer la calibration des probabilités\n",
    "- Modèles probabilistes\n",
    "- Fonction de perte pour l'entraînement\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "logloss = log_loss(y_true, y_pred_proba)\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n",
    "\n",
    "**Utilité** : Mesure de corrélation entre prédictions et vraies valeurs\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre -1 et +1\n",
    "- +1 : Prédiction parfaite\n",
    "- 0 : Prédiction aléatoire\n",
    "- -1 : Désaccord total\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Classes déséquilibrées\n",
    "- Métrique unique robuste\n",
    "- Prend en compte les 4 valeurs de la matrice de confusion\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Cohen's Kappa\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
    "\n",
    "où :\n",
    "- $p_o$ = Accuracy observée\n",
    "- $p_e$ = Accuracy attendue par hasard\n",
    "\n",
    "**Utilité** : Mesure l'accord en tenant compte du hasard\n",
    "\n",
    "**Interprétation** :\n",
    "- κ < 0 : Accord pire que le hasard\n",
    "- 0 ≤ κ < 0.2 : Accord faible\n",
    "- 0.2 ≤ κ < 0.4 : Accord moyen\n",
    "- 0.4 ≤ κ < 0.6 : Accord modéré\n",
    "- 0.6 ≤ κ < 0.8 : Accord important\n",
    "- κ ≥ 0.8 : Accord presque parfait\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Inter-annotateur agreement\n",
    "- Classes déséquilibrées\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Métriques de Régression\n",
    "\n",
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$MAE = \\frac{1}{N}\\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Utilité** : Erreur absolue moyenne\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Même unité que la variable cible\n",
    "- Robuste aux outliers\n",
    "- Ex: MAE = 5 → erreur moyenne de 5 unités\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Interprétation intuitive nécessaire\n",
    "- Présence d'outliers\n",
    "- Toutes les erreurs ont la même importance\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mean Squared Error (MSE)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Utilité** : Erreur quadratique moyenne\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Unité = (unité de y)²\n",
    "- Pénalise fortement les grandes erreurs\n",
    "- Sensible aux outliers\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Grandes erreurs doivent être pénalisées\n",
    "- Fonction de perte pour l'entraînement\n",
    "- Données sans outliers extrêmes\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} = \\sqrt{MSE}$$\n",
    "\n",
    "**Utilité** : Racine de l'erreur quadratique moyenne\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Même unité que la variable cible\n",
    "- Plus interprétable que MSE\n",
    "- Sensible aux outliers\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Mêmes cas que MSE mais avec interprétabilité\n",
    "- Comparer des modèles\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# OU\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. R² (Coefficient de Détermination)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2} = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "**Utilité** : Proportion de variance expliquée par le modèle\n",
    "\n",
    "**Interprétation** :\n",
    "- R² = 1 : Modèle parfait\n",
    "- R² = 0 : Modèle aussi bon que la moyenne\n",
    "- R² < 0 : Modèle pire que la moyenne\n",
    "- Ex: R² = 0.85 → 85% de la variance expliquée\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Évaluer la qualité globale du modèle\n",
    "- Comparer des modèles\n",
    "- Contexte de régression linéaire\n",
    "\n",
    "**Limitations** :\n",
    "- Peut être trompeur avec beaucoup de variables\n",
    "- Augmente toujours avec plus de features\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Adjusted R²\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}$$\n",
    "\n",
    "où :\n",
    "- N = nombre d'observations\n",
    "- p = nombre de variables explicatives\n",
    "\n",
    "**Utilité** : R² ajusté pour le nombre de variables\n",
    "\n",
    "**Interprétation** :\n",
    "- Pénalise l'ajout de variables non pertinentes\n",
    "- Plus conservateur que R²\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Comparer des modèles avec différents nombres de features\n",
    "- Sélection de variables\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def adjusted_r2(y_true, y_pred, n_features):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    n = len(y_true)\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    return adj_r2\n",
    "\n",
    "adj_r2 = adjusted_r2(y_true, y_pred, n_features=5)\n",
    "print(f\"Adjusted R²: {adj_r2:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$MAPE = \\frac{100\\%}{N}\\sum_{i=1}^{N} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$\n",
    "\n",
    "**Utilité** : Erreur absolue en pourcentage\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur en pourcentage\n",
    "- Indépendant de l'échelle\n",
    "- Ex: MAPE = 10% → erreur moyenne de 10%\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Comparer des modèles sur différentes échelles\n",
    "- Communication avec non-experts\n",
    "\n",
    "**Limitations** :\n",
    "- Indéfini si y = 0\n",
    "- Asymétrique (pénalise + les sous-estimations)\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "print(f\"MAPE: {mape:.4f} ({mape*100:.2f}%)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Median Absolute Error\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$MedAE = \\text{median}(|y_i - \\hat{y}_i|)$$\n",
    "\n",
    "**Utilité** : Médiane de l'erreur absolue\n",
    "\n",
    "**Interprétation** :\n",
    "- Très robuste aux outliers\n",
    "- 50% des erreurs sont inférieures à cette valeur\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Données avec outliers importants\n",
    "- Distribution d'erreurs asymétrique\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "medae = median_absolute_error(y_true, y_pred)\n",
    "print(f\"Median Absolute Error: {medae:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Explained Variance Score\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Explained Variance} = 1 - \\frac{Var(y - \\hat{y})}{Var(y)}$$\n",
    "\n",
    "**Utilité** : Proportion de variance expliquée\n",
    "\n",
    "**Interprétation** :\n",
    "- Similaire à R² mais sans le biais\n",
    "- Valeur entre 0 et 1\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "evs = explained_variance_score(y_true, y_pred)\n",
    "print(f\"Explained Variance: {evs:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Max Error\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Max Error} = \\max_{i}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Utilité** : Erreur maximale commise\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Identifier le pire cas\n",
    "- Applications critiques (sécurité)\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import max_error\n",
    "\n",
    "max_err = max_error(y_true, y_pred)\n",
    "print(f\"Max Error: {max_err:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Métriques de Clustering\n",
    "\n",
    "### 1. Silhouette Score\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n",
    "\n",
    "où :\n",
    "- a(i) = distance moyenne intra-cluster\n",
    "- b(i) = distance moyenne au cluster le plus proche\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre -1 et +1\n",
    "- s ≈ 1 : Bien clusterisé\n",
    "- s ≈ 0 : Sur la frontière\n",
    "- s < 0 : Mal assigné\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Évaluer la qualité du clustering\n",
    "- Choisir le nombre optimal de clusters\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# Score global\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Scores par échantillon\n",
    "silhouette_vals = silhouette_samples(X, cluster_labels)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Davies-Bouldin Index\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$DB = \\frac{1}{k}\\sum_{i=1}^{k}\\max_{j \\neq i}\\left(\\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\\right)$$\n",
    "\n",
    "**Utilité** : Ratio de la dispersion intra-cluster sur la séparation inter-cluster\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Plus faible = meilleur clustering\n",
    "- 0 = clusters parfaits\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Comparer différents algorithmes de clustering\n",
    "- Sélectionner le nombre de clusters\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "db_index = davies_bouldin_score(X, cluster_labels)\n",
    "print(f\"Davies-Bouldin Index: {db_index:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Calinski-Harabasz Index (Variance Ratio)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$CH = \\frac{SS_B / (k-1)}{SS_W / (N-k)}$$\n",
    "\n",
    "où :\n",
    "- SS_B = variance inter-cluster\n",
    "- SS_W = variance intra-cluster\n",
    "- k = nombre de clusters\n",
    "- N = nombre d'échantillons\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur ≥ 0\n",
    "- Plus élevé = meilleur clustering\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Clusters convexes et bien séparés\n",
    "- Rapide à calculer\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "ch_score = calinski_harabasz_score(X, cluster_labels)\n",
    "print(f\"Calinski-Harabasz Score: {ch_score:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted Rand Index (ARI)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$ARI = \\frac{RI - E[RI]}{\\max(RI) - E[RI]}$$\n",
    "\n",
    "**Utilité** : Mesure de similarité entre deux clusterings (corrigée du hasard)\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre -1 et +1\n",
    "- 1 : Accord parfait\n",
    "- 0 : Accord aléatoire\n",
    "- < 0 : Accord pire que le hasard\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Comparer avec un clustering de référence (ground truth)\n",
    "- Évaluer la stabilité du clustering\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Normalized Mutual Information (NMI)\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$NMI(U,V) = \\frac{2 \\times I(U;V)}{H(U) + H(V)}$$\n",
    "\n",
    "**Utilité** : Information mutuelle normalisée entre deux clusterings\n",
    "\n",
    "**Interprétation** :\n",
    "- Valeur entre 0 et 1\n",
    "- 1 : Accord parfait\n",
    "- 0 : Pas d'information mutuelle\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Comparer avec labels de référence\n",
    "- Robuste au nombre de clusters\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "print(f\"Normalized Mutual Information: {nmi:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Métriques Multi-Classes\n",
    "\n",
    "### Stratégies d'agrégation\n",
    "\n",
    "Pour les problèmes multi-classes, plusieurs stratégies existent :\n",
    "\n",
    "#### 1. **Macro Average**\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Macro} = \\frac{1}{C}\\sum_{c=1}^{C} \\text{Metric}_c$$\n",
    "\n",
    "**Utilité** : Moyenne non pondérée sur toutes les classes\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Toutes les classes ont la même importance\n",
    "- Classes déséquilibrées (pour ne pas favoriser les classes majoritaires)\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Weighted Average**\n",
    "\n",
    "**Formule** : \n",
    "\n",
    "$$\\text{Weighted} = \\sum_{c=1}^{C} w_c \\times \\text{Metric}_c$$\n",
    "\n",
    "où $w_c$ = proportion de la classe c\n",
    "\n",
    "**Utilité** : Moyenne pondérée par le support de chaque classe\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Classes déséquilibrées\n",
    "- Classes importantes sont plus fréquentes\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Micro Average**\n",
    "\n",
    "**Formule** : Calcul global sur tous les TP, FP, FN\n",
    "\n",
    "$$\\text{Micro Precision} = \\frac{\\sum_{c=1}^{C} TP_c}{\\sum_{c=1}^{C} (TP_c + FP_c)}$$\n",
    "\n",
    "**Utilité** : Chaque échantillon contribue également\n",
    "\n",
    "**Quand l'utiliser** :\n",
    "- Classes très déséquilibrées\n",
    "- Focus sur la performance globale\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "**Utilité** : Rapport complet avec toutes les métriques par classe\n",
    "\n",
    "**Code** :\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=['Class0', 'Class1', 'Class2'])\n",
    "print(report)\n",
    "\n",
    "# Sous forme de dictionnaire\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "```\n",
    "\n",
    "**Exemple de sortie** :\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      Class0       0.85      0.90      0.87       100\n",
    "      Class1       0.78      0.75      0.76        80\n",
    "      Class2       0.92      0.88      0.90       120\n",
    "\n",
    "    accuracy                           0.85       300\n",
    "   macro avg       0.85      0.84      0.84       300\n",
    "weighted avg       0.86      0.85      0.85       300\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliothèques et Imports\n",
    "\n",
    "### Imports complets\n",
    "\n",
    "```python\n",
    "# Bibliothèques principales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn - Métriques de classification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    zero_one_loss\n",
    ")\n",
    "\n",
    "# Métriques de régression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,  # sklearn >= 1.4\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    median_absolute_error,\n",
    "    explained_variance_score,\n",
    "    max_error,\n",
    "    mean_squared_log_error,\n",
    "    mean_poisson_deviance,\n",
    "    mean_gamma_deviance,\n",
    "    d2_absolute_error_score\n",
    ")\n",
    "\n",
    "# Métriques de clustering\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    "    fowlkes_mallows_score,\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score\n",
    ")\n",
    "\n",
    "# Visualisations\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exemple complet d'évaluation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Données d'exemple\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, \n",
    "                           weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ======== ÉVALUATION COMPLÈTE ========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MÉTRIQUES DE CLASSIFICATION BINAIRE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nMatrice de Confusion:\\n{cm}\")\n",
    "\n",
    "# Métriques de base\n",
    "print(f\"\\nAccuracy:    {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision:   {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:      {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score:    {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F2-Score:    {fbeta_score(y_test, y_pred, beta=2):.4f}\")\n",
    "\n",
    "# Métriques avancées\n",
    "print(f\"\\nROC AUC:     {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"PR AUC:      {average_precision_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"Log Loss:    {log_loss(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"MCC:         {matthews_corrcoef(y_test, y_pred):.4f}\")\n",
    "print(f\"Cohen Kappa: {cohen_kappa_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Matrice de confusion\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# 2. Courbe ROC\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba, ax=axes[1])\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Courbe Precision-Recall\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba, ax=axes[2])\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Tableau Récapitulatif : Quand Utiliser Quelle Métrique ?\n",
    "\n",
    "| Contexte | Métriques Recommandées | Raison |\n",
    "|----------|------------------------|--------|\n",
    "| **Classes équilibrées** | Accuracy, F1-Score | Simple et efficace |\n",
    "| **Classes déséquilibrées** | F1, PR-AUC, MCC | Moins sensibles au déséquilibre |\n",
    "| **Minimiser faux positifs** | Precision, F0.5 | Ex: filtrage spam |\n",
    "| **Minimiser faux négatifs** | Recall, F2 | Ex: détection médicale |\n",
    "| **Évaluation probabiliste** | ROC-AUC, Log Loss | Mesure la confiance |\n",
    "| **Régression standard** | RMSE, R² | Interprétables |\n",
    "| **Régression avec outliers** | MAE, Median AE | Robustes |\n",
    "| **Régression comparative** | MAPE | Indépendant de l'échelle |\n",
    "| **Clustering sans labels** | Silhouette, Davies-Bouldin | Mesures intrinsèques |\n",
    "| **Clustering avec labels** | ARI, NMI | Mesures extrinsèques |\n",
    "| **Multi-classes équilibrées** | Macro Average | Égalité entre classes |\n",
    "| **Multi-classes déséquilibrées** | Weighted Average | Pondération par support |\n",
    "\n",
    "---\n",
    "\n",
    "## Conseils Pratiques\n",
    "\n",
    "### 1. Validation Croisée\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Évaluation avec validation croisée\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='f1')\n",
    "print(f\"F1 scores: {scores}\")\n",
    "print(f\"Mean F1: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "\n",
    "# Plusieurs métriques\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=scoring)\n",
    "\n",
    "for metric in scoring:\n",
    "    print(f\"{metric}: {scores['test_' + metric].mean():.4f}\")\n",
    "```\n",
    "\n",
    "\n",
    "### 2. Optimisation du Seuil\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Trouver le meilleur seuil\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f\"Meilleur seuil: {best_threshold:.4f}\")\n",
    "\n",
    "# Prédictions avec nouveau seuil\n",
    "y_pred_optimized = (y_pred_proba >= best_threshold).astype(int)\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Courbe d'Apprentissage\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    model, X, y, cv=5, scoring='f1',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "### 4. Métriques Personnalisées\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    # Votre logique personnalisée\n",
    "    return np.mean((y_true == y_pred) & (y_true == 1))\n",
    "\n",
    "custom_scorer = make_scorer(custom_metric)\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Erreurs Courantes à Éviter\n",
    "\n",
    "1. **Utiliser Accuracy sur des données déséquilibrées**\n",
    "   - ❌ 99% d'accuracy peut être mauvais si 99% des données sont d'une classe\n",
    "   - ✅ Utiliser F1, MCC, ou PR-AUC\n",
    "\n",
    "2. **Oublier la validation croisée**\n",
    "   - ❌ Évaluer uniquement sur un seul split\n",
    "   - ✅ Utiliser k-fold cross-validation\n",
    "\n",
    "3. **Data leakage dans les métriques**\n",
    "   - ❌ Calculer les métriques sur les données d'entraînement\n",
    "   - ✅ Toujours évaluer sur un ensemble de test séparé\n",
    "\n",
    "4. **Ignorer le contexte métier**\n",
    "   - ❌ Optimiser uniquement pour une métrique standard\n",
    "   - ✅ Choisir la métrique selon le coût des erreurs\n",
    "\n",
    "5. **Ne pas vérifier la calibration**\n",
    "   - ❌ Utiliser des probabilités non calibrées\n",
    "   - ✅ Vérifier avec des courbes de calibration\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.title('Calibration Curve')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Ressources Supplémentaires\n",
    "\n",
    "- **Documentation Scikit-learn** : https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- **Choosing the Right Metric** : https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide\n",
    "- **ROC vs PR Curves** : https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "\n",
    "\n",
    "**Dernière mise à jour** : Février 2026\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
